# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# /// script
# requires-python = ">=3.10"
# dependencies = [
#   "cosmos-reason2-utils[inference]",
#   "datasets",
#   "vllm",
# ]
# [tool.uv.sources]
# cosmos-reason2-utils = { path = "../../cosmos_reason2_utils", editable = true }
# ///

"""Uruchamianie inferencji na zbiorze danych videophy2 w trybie offline.

üéì WYJA≈öNIENIE DLA STUDENT√ìW:
Ten skrypt analizuje wideo i ocenia, czy zjawiska fizyczne w nim pokazane sƒÖ wiarygodne.
Na przyk≈Çad, je≈õli widzimy jak pi≈Çka spada w g√≥rƒô zamiast w d√≥≈Ç, model wykryje,
≈ºe to jest niewiarygodne fizycznie.

CO ROBI TEN SKRYPT:
1. Pobiera metadane wideo z HuggingFace (bez pobierania samych wideo)
2. Przetwarza ka≈ºde wideo przez model AI Cosmos Reason2
3. Model ocenia wiarygodno≈õƒá fizycznƒÖ na skali 1-5
4. Zapisuje wyniki w plikach JSON

DLACZEGO TO JEST WA≈ªNE:
- Pomaga wykrywaƒá b≈Çƒôdy w wygenerowanych wideo (np. w grach, filmach CGI)
- Trenowanie modeli AI do rozumienia fizyki
- Kontrola jako≈õci w produkcji tre≈õci wideo

Przyk≈Çad u≈ºycia:
    uv run examples/video_critic/inference_videophy2.py --model nvidia/Cosmos-Reason2-2B
"""

from cosmos_reason2_utils.init import init_script

# üéì WYJA≈öNIENIE: init_script() przygotowuje ≈õrodowisko do pracy
# Jest to pierwszy krok przed importowaniem innych modu≈Ç√≥w
init_script()

# üéì IMPORTY - Biblioteki potrzebne do dzia≈Çania skryptu:
import argparse  # Do parsowania argument√≥w linii polece≈Ñ (--model, --output-dir, etc.)
import json      # Do zapisywania i odczytywania danych w formacie JSON
import os        # Do operacji na plikach i katalogach
import re        # Do pracy z wyra≈ºeniami regularnymi (szukanie wzorc√≥w w tek≈õcie)
import traceback # Do wy≈õwietlania szczeg√≥≈Çowych komunikat√≥w o b≈Çƒôdach
from pathlib import Path  # Do wygodnej pracy ze ≈õcie≈ºkami plik√≥w

# üéì Biblioteki zewnƒôtrzne do pracy z AI:
import datasets  # HuggingFace datasets - do pobierania zbior√≥w danych
import qwen_vl_utils  # Narzƒôdzia do przetwarzania wizji (obrazy i wideo)
import transformers  # Biblioteka do pracy z modelami transformer√≥w
import vllm  # vLLM - szybka biblioteka do inferencji LLM (Large Language Models)
import yaml  # Do czytania plik√≥w konfiguracyjnych YAML
from cosmos_reason2_utils.script.inference import Offline
from cosmos_reason2_utils.text import SYSTEM_PROMPT, create_conversation
from cosmos_reason2_utils.vision import VisionConfig

# üéì ROOT - ≈õcie≈ºka do g≈Ç√≥wnego katalogu projektu
# Path(__file__) zwraca ≈õcie≈ºkƒô do tego pliku
# .resolve() konwertuje jƒÖ na absolutnƒÖ ≈õcie≈ºkƒô
# .parent.parent.parent idzie 3 poziomy w g√≥rƒô w strukturze katalog√≥w
ROOT = Path(__file__).resolve().parent.parent.parent


def get_video_data(dataset_name: str, split: str = "train"):
    """Za≈Çaduj zbi√≥r danych i zwr√≥ƒá URL-e wideo z prawdziwymi ocenami.
    
    üéì WYJA≈öNIENIE:
    Ta funkcja pobiera TYLKO metadane (informacje o wideo) z HuggingFace,
    nie pobiera samych plik√≥w wideo. Dziƒôki temu oszczƒôdzamy czas i miejsce.
    
    Args:
        dataset_name: Nazwa zbioru danych w HuggingFace (np. "videophysics/videophy2_test")
        split: Kt√≥ra czƒô≈õƒá zbioru ("train", "test", "validation")
    
    Returns:
        Lista s≈Çownik√≥w z informacjami o wideo:
        [{
            "video_url": "https://...",  # Link do wideo
            "ground_truth": 3.5           # Prawdziwa ocena fizyczno≈õci (1-5)
        }]
    
    DLACZEGO TO TAK DZIA≈ÅA:
    - HuggingFace przechowuje zbiory danych w chmurze
    - load_dataset() pobiera tylko strukturƒô i metadane (szybko!)
    - Samo wideo jest ≈Çadowane dopiero podczas inferencji (oszczƒôdno≈õƒá pamiƒôci)
    """
    print(f"Loading dataset: {dataset_name}, split: {split}")
    
    # üéì Pobierz zbi√≥r danych z HuggingFace Hub
    dataset = datasets.load_dataset(dataset_name)
    
    # üéì Wybierz odpowiedniƒÖ czƒô≈õƒá zbioru (train/test/validation)
    dataset_split = dataset[split]

    print(
        f"Dataset loaded successfully. {split} split has {len(dataset_split)} examples."
    )

    # üéì Przygotuj listƒô z danymi wideo
    video_data = []
    for example in dataset_split:
        video_data.append(
            {
                "video_url": example["video_url"],      # URL do wideo
                "ground_truth": example["pc"],          # pc = physical correctness (poprawno≈õƒá fizyczna)
            }
        )

    # üéì Wy≈õwietl przyk≈Çadowe dane ≈ºeby zobaczyƒá strukturƒô
    if video_data:
        print(f"Sample data: {video_data[0]}")
    return video_data


def parse_answer_from_text(text: str) -> float | None:
    """Wyodrƒôbnij numerycznƒÖ odpowied≈∫ z tekstu wygenerowanego przez model.

    üéì WYJA≈öNIENIE - DLACZEGO TA FUNKCJA JEST POTRZEBNA:
    Model AI nie zwraca tylko liczby - zwraca pe≈Çny tekst z wyja≈õnieniami.
    Na przyk≈Çad mo≈ºe zwr√≥ciƒá:
    "Po przeanalizowaniu wideo widzƒô, ≈ºe fizyka jest poprawna. Ocena: 4"
    
    Ta funkcja musi znale≈∫ƒá tƒô liczbƒô "4" w ca≈Çym tek≈õcie.
    
    MO≈ªLIWE FORMATY ODPOWIEDZI:
    - Liczba sama w linii: "3" lub "4"
    - Z tekstem szablonu: "[Score between 1 and 5.]\n\n3"
    - Z wyja≈õnieniem: "3\n\nOkej, widzƒô ≈ºe..."
    
    Args:
        text: Pe≈Çna odpowied≈∫ modelu (mo≈ºe byƒá d≈Çuga)
    
    Returns:
        float: Ocena 1-5 je≈õli znaleziona
        None: Je≈õli nie uda≈Ço siƒô znale≈∫ƒá oceny
    
    DLACZEGO U≈ªYWAMY WYRA≈ªE≈É REGULARNYCH (REGEX):
    Regex pozwala znale≈∫ƒá wzorce w tek≈õcie. Wzorzec "^([1-5])\.?\s*$" oznacza:
    ^ = poczƒÖtek linii
    [1-5] = dok≈Çadnie jedna cyfra od 1 do 5
    \.? = opcjonalna kropka
    \s* = dowolna ilo≈õƒá bia≈Çych znak√≥w (spacje, tabulatory)
    $ = koniec linii
    """
    # üéì Podziel tekst na linie (ka≈ºda linia osobno)
    lines = text.strip().split("\n")

    # üéì Przeszukaj ka≈ºdƒÖ liniƒô w poszukiwaniu oceny
    for line in lines:
        line = line.strip()  # Usu≈Ñ bia≈Çe znaki z poczƒÖtku i ko≈Ñca
        
        # üéì Sprawd≈∫ czy linia zawiera pojedynczƒÖ cyfrƒô 1-5
        match = re.match(r"^([1-5])\.?\s*$", line)
        if match:
            try:
                # üéì match.group(1) zwraca pierwszƒÖ grupƒô z regex (cyfrƒô w nawiasach)
                value = float(match.group(1))
                return value
            except ValueError:
                # üéì Je≈õli konwersja siƒô nie uda, spr√≥buj nastƒôpnej linii
                continue

    # üéì Je≈õli nie znaleziono oceny w ≈ºadnej linii, zwr√≥ƒá None
    return None


def load_prompt_config(prompt_path: str) -> tuple[str, str]:
    """Za≈Çaduj konfiguracjƒô promptu z pliku YAML.
    
    üéì WYJA≈öNIENIE - CO TO JEST PROMPT:
    Prompt to instrukcja kt√≥rƒÖ dajemy modelowi AI. To jak zadanie domowe - 
    musimy jasno wyt≈Çumaczyƒá co model ma zrobiƒá.
    
    PRZYK≈ÅAD PROMPTU:
    "Obejrzyj to wideo i oce≈Ñ na skali 1-5 czy fizyka jest realistyczna.
     1 = ca≈Çkowicie nierealistyczna, 5 = bardzo realistyczna"
    
    DLACZEGO U≈ªYWAMY YAML:
    - ≈Åatwy do edycji (nie trzeba zmieniaƒá kodu!)
    - Czytelny format
    - Mo≈ºna przechowywaƒá r√≥≈ºne prompty dla r√≥≈ºnych zada≈Ñ
    
    Args:
        prompt_path: ≈öcie≈ºka do pliku YAML z promptem
    
    Returns:
        tuple: (system_prompt, user_prompt)
            - system_prompt: Og√≥lne instrukcje dla modelu (jego "rola")
            - user_prompt: Konkretne pytanie/zadanie
    
    STRUKTURA PLIKU YAML:
    system_prompt: "Jeste≈õ ekspertem od fizyki..."
    user_prompt: "Oce≈Ñ to wideo..."
    """
    # üéì Je≈õli ≈õcie≈ºka nie jest absolutna, dodaj ROOT na poczƒÖtku
    # Dziƒôki temu mo≈ºemy u≈ºywaƒá relatywnych ≈õcie≈ºek typu "prompts/video_reward.yaml"
    if not os.path.isabs(prompt_path):
        prompt_path = os.path.join(ROOT, prompt_path)

    # üéì Otw√≥rz i za≈Çaduj plik YAML
    # yaml.safe_load() parsuje YAML do s≈Çownika Pythona
    with open(prompt_path, "r") as f:
        config = yaml.safe_load(f)

    # üéì Pobierz prompty z konfiguracji, u≈ºyj domy≈õlnych je≈õli nie ma
    system_prompt = config.get("system_prompt", SYSTEM_PROMPT)
    user_prompt = config.get("user_prompt", "")

    # üéì Sprawd≈∫ czy user_prompt istnieje - jest wymagany!
    if not user_prompt:
        raise ValueError(f"No user_prompt found in {prompt_path}")

    return system_prompt, user_prompt


def run_inference_for_video(
    llm: vllm.LLM,
    processor: transformers.Qwen3VLProcessor,
    video_url: str,
    system_prompt: str,
    user_prompt: str,
    vision_kwargs: dict | None,
    sampling_params: vllm.SamplingParams,
) -> str:
    """Uruchom inferencjƒô (przewidywanie) dla pojedynczego wideo.

    üéì WYJA≈öNIENIE - CO TO JEST INFERENCJA:
    Inferencja to proces u≈ºywania wytrenowanego modelu AI do robienia przewidywa≈Ñ.
    W naszym przypadku: model "oglƒÖda" wideo i ocenia jego fizyczno≈õƒá.
    
    To jest jak egzamin - model ju≈º siƒô nauczy≈Ç, teraz sprawdzamy co potrafi.
    
    ETAPY INFERENCJI:
    1. Przygotowanie konwersacji (prompt + wideo)
    2. Przetworzenie wideo do formatu zrozumia≈Çego dla modelu
    3. Uruchomienie modelu (generowanie odpowiedzi)
    4. Zwr√≥cenie tekstu z odpowiedziƒÖ
    
    Args:
        llm: Za≈Çadowany model jƒôzykowy (Large Language Model)
        processor: Procesor do przygotowania danych wej≈õciowych
        video_url: Link do wideo do analizy
        system_prompt: Instrukcje systemowe dla modelu
        user_prompt: Konkretne pytanie u≈ºytkownika
        vision_kwargs: Parametry przetwarzania wideo (fps, rozdzielczo≈õƒá)
        sampling_params: Parametry generowania tekstu (temperatura, max_tokens)
    
    Returns:
        str: Tekstowa odpowied≈∫ modelu
    
    DLACZEGO NIE POBIERAMY WIDEO:
    vLLM potrafi ≈Çadowaƒá wideo bezpo≈õrednio z URL, wiƒôc nie musimy go pobieraƒá.
    To oszczƒôdza dysk i przyspiesza proces.
    """
    # üéì KROK 1: Utw√≥rz konwersacjƒô (format czatu)
    # Model "widzi" konwersacjƒô jak rozmowƒô:
    # System: "Jeste≈õ ekspertem..."
    # User: "Oce≈Ñ to wideo..." [wideo]
    # Assistant: [tu model generuje odpowied≈∫]
    conversation = create_conversation(
        system_prompt=system_prompt,
        user_prompt=user_prompt,
        videos=[video_url],
        vision_kwargs=vision_kwargs,
    )

    # üéì KROK 2: Przetw√≥rz dane wej≈õciowe do formatu modelu
    # add_vision_ids okre≈õla czy dodawaƒá identyfikatory dla wielu medi√≥w
    # Mamy tylko 1 wideo, wiƒôc False
    add_vision_ids = False
    prompt = processor.apply_chat_template(
        conversation,
        tokenize=False,  # Nie tokenizuj jeszcze (zostanie zrobione p√≥≈∫niej)
        add_generation_prompt=True,  # Dodaj znacznik rozpoczƒôcia generowania
        add_vision_ids=add_vision_ids,
    )

    # üéì KROK 3: Przetw√≥rz wideo
    # process_vision_info() konwertuje wideo do tensor√≥w (wielowymiarowych tablic liczb)
    # kt√≥re model potrafi przetworzyƒá
    image_inputs, video_inputs, video_kwargs = qwen_vl_utils.process_vision_info(
        conversation,
        image_patch_size=processor.image_processor.patch_size,
        return_video_kwargs=True,
        return_video_metadata=True,
    )

    # üéì KROK 4: Przygotuj dane multimedialne dla modelu
    # Model mo≈ºe przyjƒÖƒá zar√≥wno obrazy jak i wideo, wiƒôc sprawdzamy co mamy
    mm_data = {}
    if image_inputs is not None:
        mm_data["image"] = image_inputs
    if video_inputs is not None:
        mm_data["video"] = video_inputs

    # üéì KROK 5: Po≈ÇƒÖcz wszystko w jednƒÖ strukturƒô wej≈õciowƒÖ
    llm_inputs = {
        "prompt": prompt,                    # Tekst promptu
        "multi_modal_data": mm_data,        # Dane wideo
        "mm_processor_kwargs": video_kwargs, # Parametry przetwarzania
    }

    # üéì KROK 6: URUCHOM MODEL!
    # To jest moment, w kt√≥rym model "my≈õli" i generuje odpowied≈∫
    # [llm_inputs] jest listƒÖ bo vLLM mo≈ºe przetwarzaƒá wiele pr√≥bek naraz (batch)
    outputs = llm.generate([llm_inputs], sampling_params=sampling_params)

    # üéì KROK 7: Wyodrƒôbnij tekst z odpowiedzi
    # outputs[0] - pierwsza (jedyna) odpowied≈∫
    # .outputs[0] - pierwszy (jedyny) wygenerowany tekst
    # .text - czysty tekst
    # .strip() - usu≈Ñ bia≈Çe znaki z poczƒÖtku i ko≈Ñca
    output_text = outputs[0].outputs[0].text.strip()
    return output_text


def run_inference_for_dataset(args):
    """Run inference on videos for a dataset."""
    # Load video data
    print(f"Loading videos from HuggingFace dataset: {args.dataset}")
    video_data = get_video_data(args.dataset, args.split)

    print(f"\nFound {len(video_data)} videos to process")

    if not video_data:
        print("‚ùå No videos to process!")
        return

    # Use provided output directory
    output_dir = args.output_dir
    os.makedirs(output_dir, exist_ok=True)
    print(f"\nUsing output directory: {output_dir}")

    # Load prompt configuration
    prompt_path = args.input_file
    system_prompt, user_prompt = load_prompt_config(prompt_path)

    # Create Offline args with defaults (will be used for vision_kwargs and sampling_params)
    offline_args = Offline(
        model=args.model,
        revision=args.revision,
        input_file=args.input_file,
        videos=[],  # Will be set per video
        images=[],
    )

    # Set vision kwargs for video processing
    vision_kwargs = {
        "fps": 16.0,
        "total_pixels": 8192 * 28 * 28,  # 6,422,528
        "max_pixels": None,
        "max_frames": None,
    }
    # Remove None values
    vision_kwargs = {k: v for k, v in vision_kwargs.items() if v is not None}
    VisionConfig.model_validate(vision_kwargs)

    # Initialize model and processor once (reused across all videos)
    print(f"\nInitializing vLLM model: {offline_args.model}")
    llm = vllm.LLM(
        model=offline_args.model,
        revision=offline_args.revision,
        max_model_len=offline_args.max_model_len,
        limit_mm_per_prompt={"video": 1},
        enforce_eager=True,
    )
    print("‚úì Model loaded successfully")

    print("Loading processor...")
    processor: transformers.Qwen3VLProcessor = (
        transformers.AutoProcessor.from_pretrained(offline_args.model)
    )
    print("‚úì Processor loaded successfully")

    # Create sampling params for inference
    sampling_kwargs = dict(offline_args.sampling_kwargs)
    sampling_kwargs.update(
        {
            "seed": 1,
            "temperature": 0,  # Greedy decoding
            "max_tokens": 2048,
        }
    )
    # Remove None values (top_p, top_k, repetition_penalty not set)
    sampling_kwargs = {k: v for k, v in sampling_kwargs.items() if v is not None}
    sampling_params = vllm.SamplingParams(**sampling_kwargs)

    # Process each video
    for i, video_item in enumerate(video_data, 1):
        video_url = video_item["video_url"]
        ground_truth = video_item["ground_truth"]

        json_path = os.path.join(output_dir, f"{i}.json")

        if os.path.exists(json_path):
            print(
                f"\n[{i}/{len(video_data)}] üìã Results already exist: {os.path.basename(json_path)}. Skipping..."
            )
            continue

        print(f"\n[{i}/{len(video_data)}] Processing: {video_url}")

        try:
            # Run inference (reusing the same model)
            output_text = run_inference_for_video(
                llm=llm,
                processor=processor,
                video_url=video_url,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                vision_kwargs=vision_kwargs,
                sampling_params=sampling_params,
            )

            # Parse answer
            score = parse_answer_from_text(output_text)

            # Save results to JSON
            result_entry = {
                "video_url": video_url,
                "ground_truth": ground_truth,
                "output_text": output_text,
                "pred_score": score,
            }

            with open(json_path, "w") as f:
                json.dump(result_entry, f, indent=2)

            if score is not None:
                print(
                    f"‚úÖ Saved results (score: {score}) to {os.path.basename(json_path)}"
                )
            else:
                print(f"‚úÖ Saved results to {os.path.basename(json_path)}")
                print(f"   Output: {output_text[:200]}...")

        except Exception as e:
            print(f"‚ùå Error processing video: {str(e)}")
            traceback.print_exc()

    print(f"\n‚úÖ Batch processing completed. Results saved to: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description=__doc__)

    # Dataset arguments
    parser.add_argument(
        "--dataset",
        type=str,
        default="videophysics/videophy2_test",
        help='Dataset name (default: "videophysics/videophy2_test")',
    )
    parser.add_argument(
        "--split", type=str, default="test", help="Dataset split (default: test)"
    )

    # Model arguments
    parser.add_argument(
        "--model",
        type=str,
        default="nvidia/Cosmos-Reason2-2B",
        help="Model name or path (default: nvidia/Cosmos-Reason2-2B)",
    )
    parser.add_argument("--revision", type=str, default=None, help="Model revision")

    # Prompt arguments
    parser.add_argument(
        "--input-file",
        type=str,
        default="prompts/video_reward.yaml",
        help="Path to input yaml file",
    )

    # Output arguments
    parser.add_argument(
        "--output-dir",
        type=str,
        default="outputs/videophy2_test",
        help="Output directory for JSON results",
    )

    args = parser.parse_args()
    run_inference_for_dataset(args)


if __name__ == "__main__":
    main()
